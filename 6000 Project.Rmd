---
title: "STAT 6000 Project"
author: "Mark Uzochukwu"
date: "11/25/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```




```{r}
pm = read.csv("PRSA_Data_20130301-20170228/PRSA_Data_Aotizhongxin_20130301-20170228.csv", header = T)
names(pm)
View(pm)
```

## Exploring Missing values
```{r}
library(mice)
# install.packages("VIM")
library(VIM)
data(pm, package="VIM")
md.pattern(pm)
```

```{r}
x <- as.data.frame(abs(is.na(pm)))
head(pm, 10)
```

```{r}
# head(x, n=100)
```

```{r}
y = x[which(apply(x,2,sum)>0)]
cor(y)
```


##Akeem
```{r}
# sum(is.na(x))
# names(x)
cor(x[,-c(1:5,18)]) #Same task as the previous chunk.
```



```{r}
# cor(pm[,c(6:15,17)], y, use = "pairwise.complete.obs")
```



#Complete Case Analysis (Delete missing rows)

```{r}
pmcomplete = pm[complete.cases(pm),]
fit.lm = lm(PM2.5~TEMP+PRES+DEWP+RAIN+WSPM, data = x)
summary(fit.lm)
```

Multiple Imputation(MI)
```{r}
library(mice)
data(pm,package="VIM")
imp <- mice(pm, seed=1234)
fit <- with(imp, lm(PM2.5~PM10+SO2+NO2+CO+O3+TEMP+PRES+DEWP+RAIN+WSPM))
pooled <- pool(fit)
summary(pooled)
summary(imp)
```
```{r}
comp1=complete(imp, action=1)
comp1
```





## Exploratory Data Analysis (EDA)
```{r}
require(lattice)
require(ggplot2)
pairs(pmcomplete[6:15], pch = 5)
```




```{r}
wind1 <- ifelse(pmcomplete$wd == "N", 1, 0)
wind2 <- ifelse(pmcomplete$wd == "NNW", 1, 0)
wind3 <- ifelse(pmcomplete$wd == "NW", 1, 0)
wind4 <- ifelse(pmcomplete$wd == "WNW", 1, 0)
wind5 <- ifelse(pmcomplete$wd == "W", 1, 0)
wind6 <- ifelse(pmcomplete$wd == "NNE", 1, 0)
wind7 <- ifelse(pmcomplete$wd == "NE", 1, 0)
wind8 <- ifelse(pmcomplete$wd == "ENE", 1, 0)
wind = wind1+wind2+wind3+wind4+wind5+wind6+wind7+wind8
```



```{r}
pmain = cbind(pmcomplete, wind)
```

```{r}
pmmain = pmain[, c(6:15, 17, 19)]
```




```{r}
cor(pmmain)
```




```{r}
# pairs(pmmain)
```



```{r}
library(glmnet)
set.seed(6000)
n = nrow(pmmain)
trainIndex = sample(1:n, size = round(0.7*n), replace = FALSE)
train = pmmain[trainIndex, ]
test = pmmain[-trainIndex, ]
X = model.matrix(PM2.5 ~., pmmain)[,-1]
Y = pmmain$PM2.5
```



Ridge regression
```{r}
ridge.mod = glmnet(X[trainIndex, ], Y[trainIndex], alpha =0)
```

With cross validation
```{r}
library(boot)
set.seed(1)
cvout = cv.glmnet(X[trainIndex, ], Y[trainIndex], alpha = 0)
bestlam0 = cvout$lambda.min
bestlam0
```
The test error associated to best lambda is with cross validation is
```{r}
ridge.pred = predict(ridge.mod, s=bestlam0, newx=X[-trainIndex, ])
ridgeerror = mean((ridge.pred - Y[-trainIndex])^2)
ridgeerror
```

```{r}
coef(ridge.mod)[,bestlam0]
```



## Lasso regression
```{r}
lasso.mod = glmnet(X[trainIndex, ], Y[trainIndex], alpha =1)
```

With cross validation
```{r}
library(boot)
set.seed(1)
cvout1 = cv.glmnet(X[trainIndex, ], Y[trainIndex], alpha = 1)
bestlam1 = cvout1$lambda.min
bestlam1
```
The test error associated to best lambda is with cross validation is
```{r}
lasso.pred = predict(lasso.mod, s = bestlam1, newx=X[-trainIndex, ])
lassoerror = mean((lasso.pred - Y[-trainIndex])^2)
lassoerror
```

The Coefficients.

```{r}
out=glmnet(X,Y,alpha =1, lambda =)
lassocoef= predict (out ,type ="coefficients",s=bestlam1 )[1:11,]
lassocoef
```

There are only four (4) nonzero coefficient estimates.


# Decision Trees

## Bagging & Random Forest

```{r}
library(randomForest)
set.seed(6000)
smpl.size <- floor(0.5*nrow(pmmain))
train <- sample(seq_len(nrow(pmmain)), size = smpl.size)
smp.test <- pmmain[-train, ]
pm2.5.test<-PM2.5[-train]

bag.pm<- randomForest(PM2.5~.,data = pmmain, subset = train, mtry = 11, importance = TRUE) #m=p
bag.pm

#Testing error/Performance accuracy of bagging

yhat.bag<-predict(bag.pm, newdata = smp.test)
plot(yhat.bag, pm2.5.test)
abline(0,1, col = 2, lwd = 2)
mean((yhat.bag - pm2.5.test)^2) #test MSE of Bagging


#RandomForest
# By default, randomForest() uses p/3 variables when building a random forest of regression trees, and √p variables when building a random forest of classiﬁcation trees. 

#Using p = 5

set.seed(6000)
rf.pm<- randomForest(PM2.5~.,data = pmmain, subset = train, mtry = 5, importance = TRUE) 
yhat.rf<-predict(rf.pm, newdata = smp.test)
mean((yhat.rf - pm2.5.test)^2)

importance(rf.pm)

varImpPlot(rf.pm)
```

## Boosting

```{r}
# install.packages("gbm")
library(gbm)
set.seed(6000)
boost.pm<- gbm(PM2.5~.,data = pmmain[train,], distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
summary(boost.pm)

par(mfrow=c(1,2))
plot(boost.pm ,i="CO")
plot(boost.pm ,i="NO2")

#Testing error/Performance accuracy of boosting

yhat.boost<-predict(boost.pm, newdata = smp.test, n.trees = 5000)

classVariables = sapply(pmmain, function(x) class(x))
classVariables

mean((yhat.boost - pm2.5.test)^2) #MSE = 424.1 when n.trees = 4000, and MSE= 423.3 when n.trees = 5000.

#Is there a means of finding covergence point like a monte-carlo analysis?

```





