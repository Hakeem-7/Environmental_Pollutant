---
title: "STAT 6000 Project"
author: "Mark Uzochukwu"
date: "11/25/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```




```{r}
pm = read.csv("PRSA_Data_20130301-20170228/PRSA_Data_Aotizhongxin_20130301-20170228.csv", header = T)
names(pm)
View(pm)
```

## Exploring Missing values
```{r}
library(mice)
# install.packages("VIM")
library(VIM)
data(pm, package="VIM")
md.pattern(pm)
```

```{r}
x <- as.data.frame(abs(is.na(pm)))
head(pm, 10)
```

```{r}
# head(x, n=100)
```

```{r}
y = x[which(apply(x,2,sum)>0)]
cor(y)
```


##Akeem
```{r}
# sum(is.na(x))
# names(x)
cor(x[,-c(1:5,18)]) #Performs same task as the previous chunk.
```



```{r}
# cor(pm[,c(6:15,17)], y, use = "pairwise.complete.obs")
```



#Complete Case Analysis (Delete missing rows)

```{r}
pmcomplete = pm[complete.cases(pm),] # To reduce strings in the worksheet, fine tune the original string as shown below. 
pm<-na.omit(pm) #same result as using "pmcomplete."

fit.lm = lm(PM2.5~TEMP+PRES+DEWP+RAIN+WSPM, data = pm) #Other pollutants were excluded from the linear regression model.
summary(fit.lm) 
```


## Multiple Imputation(MI)

```{r}
library(mice)
data(pm,package="VIM")
imp <- mice(pm, seed=1)
fit <- with(imp, lm(PM2.5~PM10+SO2+NO2+CO+O3+TEMP+PRES+DEWP+RAIN+WSPM))
pooled <- pool(fit)
summary(pooled)
summary(imp)
```


```{r}
comp1=complete(imp, action=1)
comp1 #I'd suggest you add comments to the multiple imputation analysis for future reference.
```



```{r}
wind1 <- ifelse(pm$wd == "N", 1, 0)
wind2 <- ifelse(pm$wd == "NNW", 1, 0)
wind3 <- ifelse(pm$wd == "NW", 1, 0)
wind4 <- ifelse(pm$wd == "WNW", 1, 0)
wind5 <- ifelse(pm$wd == "W", 1, 0)
wind6 <- ifelse(pm$wd == "NNE", 1, 0)
wind7 <- ifelse(pm$wd == "NE", 1, 0)
wind8 <- ifelse(pm$wd == "ENE", 1, 0)
wind = wind1+wind2+wind3+wind4+wind5+wind6+wind7+wind8
# View(wind)
```



```{r}
pm.v1 = cbind(pm, wind)
# names(pm.v1)
pm.v2 = pm.v1[,-c(1:5,7:11,16,18)]
# View(pm.v2)
cor(pm.v2)
```


##Sample Splitting

```{r}
attach(pm.v2)
library(glmnet)
set.seed(6000)
n = nrow(pm.v2)
trainIndex = sample(1:n, size = round(0.7*n), replace = FALSE)
train = pm.v2[trainIndex, ]
test = pm.v2[-trainIndex, ]
X = model.matrix(PM2.5 ~., pm.v2)[,-1]
Y = pm.v2$PM2.5
```


## Linear Regression

```{r}
fit.lm = lm(PM2.5~., data = pm.v2) #Other pollutants were excluded from the linear regression model.
m = summary(fit.lm)

#MSE computation

mse <- function(m){
  cbind("MSE of the Linear Regression Model is = ", round(mean(m$residuals^2), 0))
}
mse(m)

# Linear regression model suggests that "Temp" and "Wind Direction" have the greatest impact on the pollutatant pm2.5
```



## Ridge regression

```{r}
ridge.mod = glmnet(X[trainIndex, ], Y[trainIndex], alpha =0)

#With cross validation

library(boot)
set.seed(1)
cvout = cv.glmnet(X[trainIndex, ], Y[trainIndex], alpha = 0)
bestlam0 = cvout$lambda.min
bestlam0

#Accuracy evaluation

ridge.pred = predict(ridge.mod, s=bestlam0, newx=X[-trainIndex, ])
round(mean((ridge.pred - Y[-trainIndex])^2),0)

#MSE is approx 5000.

coef(ridge.mod)[,bestlam0]
```



## Lasso regression
```{r}
lasso.mod = glmnet(X[trainIndex, ], Y[trainIndex], alpha =1)

#With cross validation

library(boot)
set.seed(1)
cvout1 = cv.glmnet(X[trainIndex, ], Y[trainIndex], alpha = 1)
bestlam1 = cvout1$lambda.min
bestlam1

# Prediction Accuracy


lasso.pred = predict(lasso.mod, s = bestlam1, newx=X[-trainIndex, ])
round(mean((lasso.pred - Y[-trainIndex])^2),0)

#MSE is approx 4971.

#Coefficients

out=glmnet(X,Y,alpha =1, lambda = bestlam1)
lassocoef= predict (out ,type ="coefficients",s=bestlam1 )[1:7,]
lassocoef #This suggests that all meteorological predictors considered in the regression analysis are relevant.
```



# Decision Trees: Bagging,Random Forest, and Boosting

```{r}
library(randomForest)
set.seed(6000)
smpl.size <- floor(0.5*nrow(pm.v2))
train <- sample(seq_len(nrow(pm.v2)), size = smpl.size)
smp.test <- pm.v2[-train, ]
pm2.5.test<-PM2.5[-train]

bag.pm<- randomForest(PM2.5~.,data = pm.v2, subset = train, mtry = 6, importance = TRUE) #m=p
bag.pm

#Testing error/Performance accuracy of bagging

yhat.bag<-predict(bag.pm, newdata = smp.test)
plot(yhat.bag, pm2.5.test)
abline(0,1, col = 2, lwd = 2)
round(mean((yhat.bag - pm2.5.test)^2),0) #test MSE of Bagging is 3605



#RandomForest

# By default, randomForest() uses p/3 variables when building a random forest of regression trees, and √p variables when building a random forest of classiﬁcation trees. 

#Using p = 3

set.seed(6000)
rf.pm<- randomForest(PM2.5~.,data = pm.v2, subset = train, mtry = 3, importance = TRUE) 
rf.pm
yhat.rf<-predict(rf.pm, newdata = smp.test)
round(mean((yhat.rf - pm2.5.test)^2), 0) #test MSE of Random Forest is 3569

importance(rf.pm)

varImpPlot(rf.pm)


library(ggplot2)
library(magrittr)
library(dplyr) #Learn more about this library
# feat_imp_df<- importance(rf.pm) %>% #Piping a dataframe from VarImpPlot of Random Forest.
#   data.frame() %>%
#   mutate(feature = row.names(.))
# 
# #dataframe plot
# ggplot(feat_imp_df, aes(x = reorder(feature, MeanDecreaseGini), 
#                         y = MeanDecreaseGini)) + 
#   geom_bar(stat='identity') +
#     coord_flip() +
#     theme_classic() +
#     labs(
#       x     = "Feature",
#       y     = "Importance",
#       title = "Feature Importance: <Model>"
#     )


```

## Boosting

```{r}
# install.packages("gbm")
library(gbm)
set.seed(6000)
boost.pm<- gbm(PM2.5~.,data = pm.v2[train,], distribution = "gaussian", n.trees = 2000, interaction.depth = 4)
summary(boost.pm)

par(mfrow=c(1,2))
plot(boost.pm ,i="TEMP")
plot(boost.pm ,i="PRES")

#Testing error/Performance accuracy of boosting

yhat.boost<-predict(boost.pm, newdata = smp.test, n.trees = 2000)

classVariables = sapply(pm.v2, function(x) class(x))
classVariables

mean((yhat.boost - pm2.5.test)^2) #MSE = 3883 when n.trees = 4000, and MSE= 423.3 when n.trees = 2000 

#Is there a means of finding covergence point like a monte-carlo analysis to determine the optimum ntrees?

```





